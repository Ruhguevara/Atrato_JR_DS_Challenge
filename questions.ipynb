{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:0.02in solid gray\"> </hr>\n",
    "\n",
    "<center> <font color= #847ACC> <h1> Atrato JR Data Scientist Challenge <center></h1> </font>\n",
    "    \n",
    "<center> <font color= #847ACC> <font size = 4>  Rubén Hernández Guevara <center> </font> \n",
    "<br>\n",
    "<center> Repository: https://github.com/Ruhguevara/Atrato_JR_DS_Challenge\n",
    "<br>\n",
    "<br>\n",
    "<hr style=\"border:0.02in solid gray\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a basic understanding of Machine Learning algorithms and their applications. Please answer the following questions:\n",
    "\n",
    "**a. Describe briefly each of the following algorithms and its typical use cases:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Logistic Regression.**\n",
    "\n",
    "    _`How the weighted combination of input features, transformed by the logistic function, provides a probabilistic framework for effective binary classification.`_\n",
    "\n",
    "    Logistic Regression is a relatively simple algorithm; it transforms a linear input into a probability (in the range of 0-1) using the Sigmoid function:\n",
    "\n",
    "    $$S(x) = \\frac{1}{1+e^{-X\\beta}}$$\n",
    " \n",
    "    Where $X$ is the set of predictor features, and $\\beta$ is the corresponding weight vector. Computing $S(x)$ yields a probability indicating whether an observation should be classified as `1` or `0`.\n",
    "\n",
    "    - It is highly interpretable due to its output (probabilities), making it easier to explain to a non-technical audience compared to other models.\n",
    "\n",
    "    - Computationally, it requires less computational power compared to more complex models such as Neural Networks or Decision Trees.\n",
    "\n",
    "    - Its relative simplicity makes it a high-bias and low-variance model, so it may not perform well when the decision bounday is not linear.\n",
    "\n",
    "    - When features are highly correlated, the coefficients $\\beta$  won't be as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multiple Linear Regression.**\n",
    "\n",
    "    _``How the combined influence of multiple predictors, guided by optimal coefficients, enhances the predictive power of the model.``_\n",
    "    \n",
    "    The formula is very similar to a simple Linear Regression:\n",
    "\n",
    "    $$y = \\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n + \\epsilon$$\n",
    "\n",
    "    Where:\n",
    "    - $y =$  The predicted value of the independet variable.\n",
    "    - $B_0 =$ The y-intercept  (value of $y$ when all other parameters are set to 0).\n",
    "    - $\\beta_1X_1 =$ The regression coefficient ($\\beta_1$) of the first independet variable ($X_1$)  (the effect that increasing the value of the independent variable has on the predicted y value).\n",
    "    - $... = $ Do the same for however many independent variables are being tested.\n",
    "    - $\\beta_n X_n =$ The regression coefficient of the last independet variable.\n",
    "    - $\\epsilon =$ Model error (how much variation there is in the estimate of y).\n",
    "\n",
    "    Multiple linear regression calculates $R^2$ and $p-values$ from the sum of the Squared Residuals (SSR). And the residuals are the difference between observed objective variable and predicted variable. The difference between linear regression is that Multiple Linear Regression calculates residuals around a fitted plane instead of a line:\n",
    "\n",
    "    $$R^2 = \\frac{SSR(mean) - SSR(fitted plane)}{SSR(mean)}$$\n",
    "\n",
    "    Use cases: when there are 2 or more variables, for example, when trying to predict Height, variables like weight, age, shoe size, etc, can be used in a Multiple Linear Regression.\n",
    "    \n",
    "    [1] Starmer, J. (2022). Linear Regression. In _The StatQuest Illustrated Guide to Machine Learning_ (pp. 75-83). Multiple Linear Regression (p. 80).\n",
    "    \n",
    "    [2] Bevans, R. (2023, June 22). Multiple Linear Regression | A Quick Guide (Examples). Scribbr. Retrieved November 21, 2023, from https://www.scribbr.com/statistics/multiple-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Forest**\n",
    "\n",
    "    _``How the majority vote and well-placed randomness can enhance the decision tree model.``_\n",
    "\n",
    "    Random forest starts with two important concepts: \n",
    "\n",
    "    - *decision trees:* It's an algorithm based on decision, where each node of a tree would represent a decision, and it can be trees for classification and regression. Typically, an individual decision tree may be prone to overfitting because a leaf node can be created for each observation.\n",
    "    \n",
    "    - *ensemble learning:* Consists of multiple models working together to come to an aggregate prediction.\n",
    "\n",
    "    Random Forest is an example of ensemble learning where each model is a decision tree, i.e., a different model. It produces aggregated predictions using the predictions from several decision trees. \n",
    "\n",
    "    Two characteristics of Random Forests allow a reduction in overfitting and the correlation between the trees:\n",
    "    \n",
    "    - Bagging significantly reduces the variance of the random forest versus the variance of any individual decision trees.\n",
    "    - A random subset of features is considered at each split, preventing the important features from always being present at the tops of individual trees.\n",
    "\n",
    "    Use cases: Classification and Regression tasks.\n",
    "\n",
    "    [1] Yeon, J. Wilber, J. _The Random Forest Algorithm_. MLU-EXPLAIN. Retrieved November 21, 2023, from https://mlu-explain.github.io/random-forest/\n",
    "    \n",
    "    [2] Huo, K. Singh, N. (2021). Machine Learning. In _Ace the Data Science Interview_ (pp. 77-140). Random Forests (pp. 98-99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost**\n",
    "\n",
    "    _``How boosting and optimized gradient boosting techniques can supercharge decision tree models.``_\n",
    "\n",
    "    XGBoost stands for Extreme Gradient Boosting, it is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the [Gradient Boosting framework](https://en.wikipedia.org/wiki/Gradient_boosting).\n",
    "    \n",
    "    It provides parallel tree boosting and its use cases are: regression, classification, and ranking problems.\n",
    "\n",
    "    XGBoost is build upon supervised machine learning, decision trees, ensemble learning and gradient boosting.\n",
    "\n",
    "    - *gradient boosting:* Is a machine learning technique used in regression and classification tasks. It gives a prediction model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about the data, which are typically simple decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees.\n",
    "\n",
    "    Three main forms of gradient boosting are supported:\n",
    "\n",
    "    Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "\n",
    "    Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\n",
    "    \n",
    "    Regularized Gradient Boosting with both L1 and L2 regularization.\n",
    "\n",
    "    [1] NVIDIA. _XGBoost_. NVIDIA. Retrieved November 21, 2023, from https://www.nvidia.com/en-us/glossary/data-science/xgboost/\n",
    "    \n",
    "    [2] Brownlee, J. (2021). _A Gentle Introduction to XGBoost for Applied Machine Learning_. Machine Learning Mastery. Retrieved November 21, 2023, from https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Genetic Algorithms**\n",
    "\n",
    "    _``How evolution-inspired selection and genetic diversity improve optimization and search algorithms.``_\n",
    "\n",
    "    Genetic algorithms are a family of search algorithms inspired by the principles of evolution\n",
    "    in nature. By imitating the process of natural selection and reproduction, genetic algorithms\n",
    "    can produce high-quality solutions for various problems involving search, optimization,\n",
    "    and learning. At the same time, their analogy to natural evolution allows genetic\n",
    "    algorithms to overcome some of the hurdles that are encountered by traditional search and\n",
    "    optimization algorithms, especially for problems with a large number of parameters and\n",
    "    complex mathematical representations.\n",
    "\n",
    "    Genetic algorithms implement a simplified version of the Darwinian evolution that takes\n",
    "    place in nature. The principles of the Darwinian evolution theory can be summarized using\n",
    "    the following principles:\n",
    "\n",
    "    - Variation\n",
    "    - Inheritance\n",
    "    - Selection\n",
    "\n",
    "    [1] Wirsansky, E. (2020). The Basics of Genetic Algorithms. In _Hands-On Genetic Algorithms with Python_. (pp. 8-23). What are genetic algorithms? (pp. 9-10).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **KMeans**\n",
    "\n",
    "    _``How iterative clustering and centroid-based randomness can enhance data grouping.``_\n",
    "\n",
    "    K-Means is the most common clustering approach wich assumes that the closer data points are to each other, the more simmilar they are. It determines K clusters based on Euclidean distances. It's an _Unsupervised ML model_.\n",
    "\n",
    "    Some of its advantages are:\n",
    "    - Scales to large datasets\n",
    "    - Interpretable & explainable results\n",
    "    - Can generate tight clusters\n",
    "\n",
    "    Disadvantages:\n",
    "    - Requires defining the expected number of clusters in advance\n",
    "    - Not suitable to identify clusters with non-convex shapes\n",
    "\n",
    "    Some examples of business-oriented applications of clustering include the grouping of documents, \n",
    "    music, and movies by different topics, or finding customers that share similar interests based on \n",
    "    common purchase behaviors as a basis for recommendation engines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What is the difference between supervised and unsupervised learning algorithms?**\n",
    "\n",
    "- Describe the types of supervised and unsupervised learning\n",
    "\n",
    "    - Supervised Learning involes a human \"teacher\" or \"supervisor\". Their role is to feed the computer with labeled data or examples consisting of a combination of problems and solutions. Supervised machine learning is used for two types of problems or tasks:\n",
    "    \n",
    "        - Classification\n",
    "        - Regression\n",
    "    <br>\n",
    "    <br>\n",
    "    - Unsupervised Learning is used when there is no labeled data or instructions for the computer to follow. Instead, the computer tries to identify the underlying structure or patterns in the data without any assistance. Unsupervised learning is used for three main tasks:\n",
    "\n",
    "        - Clustering\n",
    "        - Association\n",
    "        - Dimensionality Reduction\n",
    "\n",
    "        In each of these tasks, we want to discover the inherent structure of our data for which no predefined categories or labels exist.\n",
    "\n",
    "    [1] Nikolopoulou, K. (2023, August 21). _Supervised vs. Unsupervised Learning: Key Differences_. Scribbr. Retrieved November 21, 2023, from https://www.scribbr.com/ai-tools/supervised-vs-unsupervised-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Are you familiar with any techniques for algorithm evaluation and performance metrics?**\n",
    "\n",
    "Yes, I have previously worked with these, Ill include a brief description:\n",
    "\n",
    "- **Cross-Validation:**\n",
    "\n",
    "    K-Fold Cross-Validation: The dataset is divided into k subsets, and the model is trained and evaluated k times, using a different subset for testing in each iteration.\n",
    "\n",
    "- **Performance Metrics for Classification:**\n",
    "\n",
    "    - Accuracy: The proportion of correctly classified instances\n",
    "    - Precision: The ratio of true positives to the total predicted positives\n",
    "    - Recall (Sensitivity): The ratio of true positives to the total actual positives\n",
    "    - F1 Score: The harmonic mean of precision and recall\n",
    "    - Confusion Matrix: A table showing the true positives, true negatives, false positives, and false negatives\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Performance Metrics for Regression:**\n",
    "\n",
    "    - Mean Absolute Error (MAE): The average absolute differences between predicted and actual values\n",
    "    - Mean Squared Error (MSE): The average of squared differences between predicted and actual values\n",
    "    - Root Mean Squared Error (RMSE): The square root of the MSE, providing the error in the same units as the target variable\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC):**\n",
    "\n",
    "    Commonly used for binary classification problems, the ROC curve illustrates the trade-off between true positive rate and false positive rate. AUC quantifies the area under the ROC curve, providing a single-value summary of classifier performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "\n",
    "    Especially useful when dealing with imbalanced datasets, the precision-recall curve illustrates the trade-off between precision and recall at different decision thresholds.\n",
    "\n",
    "- **Cross-Validation Metrics:**\n",
    "\n",
    "    Use cross-validation results to compute average performance metrics, providing a more robust estimate of model performance.\n",
    "\n",
    "- **Learning Curves:**\n",
    "\n",
    "    Plotting performance metrics against the size of the training dataset helps identify underfitting or overfitting issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. Observe the distribution plot of the followig variable:**\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"imgs/dist_plot.png\" width=\"400\" height=\"260\"></div>\n",
    "\n",
    "- What can you infer about the data's asymmetry? Would you say the distribution is symmetric, skewed to the right (positively skewed), or skewed to the left (negatively skewed)? Please provide a brief explanation of your answer based on the shape of the plot.\n",
    "\n",
    "    - The data is skewed to the right (positively skewed) because the tail of the distribution extends more to the right side. \n",
    "\n",
    "- In the distribution plot, what can you infer about the data's concentration? Does the distribution exhibit a pronounced peak or is it more flattened?\n",
    "\n",
    "    - The distribution does exhibit a pronounced peak in the left side, this means that there is a relatively small number of very high values pulling the mean to the right of the peak of the data. The median and mode of the distribution are typically less than the mean.\n",
    "\n",
    "In a different enviroment, with the data available, I would use a function to identify the distribution of the data in order to get better insights.\n",
    "With the library `Fitter` it is possible to do so withouth using too much computational resources.\n",
    "\n",
    "[1] Turney, S. (2023, November 10). Skewness | Definition, Examples & Formula. Scribbr. Retrieved November 21, 2023, from https://www.scribbr.com/statistics/skewness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra points:**\n",
    "\n",
    "**a. Are you familiar with any technique to reduce bias in the distribution of a variable? If so, please mention the technique.**\n",
    "\n",
    "Yes, I have worked with these methods:\n",
    "\n",
    "- Random Sampling: Ensure that the sample is randomly selected from the population. This helps in making the sample representative of the population.\n",
    "\n",
    "- Stratified Sampling:  If the population has distinct subgroups (strata), use stratified sampling to ensure that each subgroup is proportionally represented in the sample.\n",
    "\n",
    "- Weighting: Apply weights to your data to adjust for unequal representation or sampling probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**b. How would you apply the technique to a DataFrame in Python?**\n",
    "\n",
    "- Random Sampling: ``random_sample = df.sample(n=10)``\n",
    "\n",
    "- Stratified Sampling: ``strat_sample, _ = train_test_split(df, test_size=0.8, stratify=df['Group'])``\n",
    "\n",
    "- Weighting: \n",
    "\n",
    "    ``group_weights = df['Group'].value_counts(normalize=True).to_dict()``\n",
    "\n",
    "    ``df['Weight'] = df['Group'].apply(lambda x: 1 / group_weights[x])``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra points:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to integrate DVC in the pipeline\n",
    "\n",
    "    - install dvc with `pip install dvc`.\n",
    "\n",
    "    - initialize dvc with `dvc init`. This will create a .dvc folder to track configurations\n",
    "\n",
    "    - Set up remote storage `dvc remote add -d myremote s3/google storage/azure`\n",
    "\n",
    "    - Track data `dvc add data.csv`.\n",
    "\n",
    "    - Push data to remote storage `dvc push`.\n",
    "\n",
    "    - pull data when needed `dvc pull`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to integrate MLFlow in the pipeline\n",
    "\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you explain and give examples about this following concepts:\n",
    "\n",
    "    - Encapsulation\n",
    "    \n",
    "        - Restricted or controlled access to data.\n",
    "        - In a 'Car' class, there might be private variables like engineSpeed and fuelLevel. We can provide public methods like accelerate() and checkFuel() to interact with these variables, but the direct manipulation of engineSpeed and fuelLevel from outside the class is prohibited.\n",
    "\n",
    "    - Abstraction\n",
    "\n",
    "        - Represents a design instead of a concrete instance or implementation.\n",
    "        - 'Smartphone' class. It has methods like makeCall() or sendText(). When you use these methods, you don't need to know the underlying implementation of how a call is made or a text is sent. The complexity of the cellular network operations is abstracted away from the user.\n",
    "\n",
    "    - Inheritance\n",
    "\n",
    "        - Shared design & interfaces among classes. Enables subtypes and AST (abstract syntax trees).\n",
    "        - class 'Vehicle' with properties like speed and fuelCapacity. You can create derived classes like Bike and Car that inherit these properties and perhaps add more specific features like numWheels or airConditioning specific to each type of vehicle.\n",
    "\n",
    "    - Polymorphism\n",
    "        - One interface on many forms (types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# class Bird:\n",
    "#    def sing(self):\n",
    "#        return \"Tweet\"\n",
    "\n",
    "# class Duck(Bird):\n",
    "    def sing(self):\n",
    "        return \"Quack\"\n",
    "\n",
    "# Polymorphism\n",
    "# bird = Bird()\n",
    "# duck = Duck()\n",
    "\n",
    "# print(bird.sing())  # Outputs: Tweet\n",
    "# print(duck.sing())  # Outputs: Quack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, both Bird and Duck have a sing method. The method's behavior changes depending on whether it's called on a Bird or a Duck object, demonstrating polymorphism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
